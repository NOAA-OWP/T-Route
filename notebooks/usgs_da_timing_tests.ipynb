{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "\n",
    "routelink_subset_file = \"../test/input/geo/wrf-hydro-run/DOMAIN/routeLink_subset.nc\"\n",
    "\n",
    "usgs_timeslices_folder = \"../test/input/geo/wrf-hydro-run/nudgingTimeSliceObs/\"\n",
    "\n",
    "usgs_file_pattern_filter = \"2020-03*.usgsTimeSlice.ncdf\"\n",
    "\n",
    "usgs_files = glob.glob(usgs_timeslices_folder + usgs_file_pattern_filter)\n",
    "\n",
    "len(usgs_files), len(glob.glob(routelink_subset_file))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST our Function on a single Datset\n",
    "ds = xr.open_dataset(usgs_files[0])\n",
    "# ds.stationId.values.bytes.strip()\n",
    "\n",
    "stationId_da = map(bytes.strip, ds.stationId.values)\n",
    "# list(map(bytes.isdigit, stationId_da))\n",
    "from compose import compose\n",
    "\n",
    "stationId_da_mask = list(\n",
    "    map(bytes.isdigit, stationId_da)\n",
    ")  # Make a mask to remove the blank values\n",
    "stationId_da_mask = list(\n",
    "    map(compose(bytes.isdigit, bytes.strip), ds.stationId.values)\n",
    ")  # Make a mask to remove the blank values\n",
    "stationId = ds.stationId[stationId_da_mask].values.astype(int)\n",
    "\n",
    "unique_times = np.unique(ds.time.values)\n",
    "for i, t in enumerate(unique_times):\n",
    "    unique_times[i] = t.replace(b\"_\", b\"T\")\n",
    "\n",
    "data_vars = {}\n",
    "for v in (\"discharge\", \"discharge_quality\"):\n",
    "    data_vars[v] = ([\"stationId\"], ds[v][stationId_da_mask].values)\n",
    "xr.Dataset(data_vars=data_vars, coords={\"stationId\": stationId, \"time\": unique_times})\n",
    "stationId_da = list(map(bytes.strip, ds.stationId.values))\n",
    "stationId_da_mask = list(\n",
    "    map(bytes.isdigit, stationId_da)\n",
    ")  # Make a mask to remove the blank values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_time_station_index(xd):\n",
    "\n",
    "    stationId_da_mask = list(\n",
    "        map(compose(bytes.isdigit, bytes.strip), xd.stationId.values)\n",
    "    )\n",
    "    stationId = xd.stationId[stationId_da_mask].values.astype(int)\n",
    "\n",
    "    unique_times_str = np.unique(xd.time.values).tolist()\n",
    "\n",
    "    # Three different ways to do the dates\n",
    "    # 1) as strings\n",
    "    unique_times = np.array(unique_times_str, dtype=\"str\")\n",
    "\n",
    "    # 2) as strings, but a different way\n",
    "    #     unique_times = np.unique(xd.time.values)\n",
    "    # 3) as dates\n",
    "    #     for i, t in enumerate(unique_times_str):\n",
    "    #         unique_times_str[i] = t.replace(b\"_\",b\"T\")\n",
    "    #     unique_times = np.array(unique_times_str,dtype=\"datetime64\")\n",
    "\n",
    "    data_var_dict = {}\n",
    "    data_vars = (\"discharge\", \"discharge_quality\")\n",
    "    for v in data_vars:\n",
    "        data_var_dict[v] = ([\"stationId\"], xd[v].values[stationId_da_mask])\n",
    "    return xr.Dataset(\n",
    "        data_vars=data_var_dict, coords={\"stationId\": stationId, \"time\": unique_times}\n",
    "    )\n",
    "\n",
    "\n",
    "# return xr.Dataset(data_vars=data_vars{'discharge': (['stationId'], xd.discharge.values), 'discharge_quality': (['stationId'], xd.discharge_quality.values)},\n",
    "# coords={'stationId': stationId, 'time': np.unique(xd.time.values)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "#%%prun\n",
    "# How long to preprocess one dataset?\n",
    "preprocess_time_station_index(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# %%prun\n",
    "xr.open_mfdataset(\n",
    "    usgs_files[0 : len(usgs_files) // 1],\n",
    "    preprocess=preprocess_time_station_index,\n",
    "    combine=\"by_coords\",\n",
    "    # data_vars=\"minimal\",\n",
    "    # coords=\"minimal\",\n",
    "    # compat=\"override\",\n",
    "    # chunks= {'time':481,'stationId':1} #3869*481,\n",
    "    parallel=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "# %%prun\n",
    "# This version appears to be the fastest\n",
    "def read_netcdfs(files, dim, transform_func=None):\n",
    "    def process_one_path(path):\n",
    "        # use a context manager, to ensure the file gets closed after use\n",
    "        with xr.open_dataset(path) as ds:\n",
    "            # transform_func should do some sort of selection or\n",
    "            # aggregation\n",
    "            if transform_func is not None:\n",
    "                ds = transform_func(ds)\n",
    "            # load all data from the transformed dataset, to ensure we can\n",
    "            # use it after closing each original file\n",
    "            ds.load()\n",
    "            return ds\n",
    "\n",
    "    paths = sorted(glob.glob(files))\n",
    "    datasets = [process_one_path(p) for p in paths]\n",
    "    combined = xr.concat(datasets, dim)\n",
    "    return combined\n",
    "\n",
    "\n",
    "# you might also use indexing operations like .sel to subset datasets\n",
    "# combined = read_netcdfs('/all/my/files/*.nc', dim='time',\n",
    "#                         transform_func=lambda ds: ds.mean())\n",
    "\n",
    "with read_netcdfs(\n",
    "    usgs_timeslices_folder + usgs_file_pattern_filter,\n",
    "    \"time\",\n",
    "    preprocess_time_station_index,\n",
    ") as ds2:\n",
    "    df2 = pd.DataFrame(\n",
    "        ds2[\"discharge\"].values.T,\n",
    "        index=ds2[\"stationId\"].values,\n",
    "        columns=ds2.time.values,\n",
    "    )\n",
    "\n",
    "df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "with xr.open_mfdataset(\n",
    "    usgs_files, preprocess=preprocess_time_station_index, combine=\"by_coords\"\n",
    ") as ds2:\n",
    "    df3 = pd.DataFrame(\n",
    "        ds2[\"discharge\"].values.T,\n",
    "        index=ds2[\"stationId\"].values,\n",
    "        columns=ds2.time.values,\n",
    "    )\n",
    "\n",
    "df3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_t = xr.open_dataset(usgs_files[0])\n",
    "ds_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = ds_t.set_coords([\"stationId\", \"time\"])\n",
    "xs = xs.set_index(stationIdInd=\"stationId\")\n",
    "# xr.Dataset(data_vars={\"discharge\": da_discharge})\n",
    "xs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the Route-Link File to convert the indexes\n",
    "with xr.open_dataset(routelink_subset_file) as ds:\n",
    "    gage_list = list(map(bytes.strip, ds.gages.values))\n",
    "    gage_mask = list(map(bytes.isdigit, gage_list))\n",
    "\n",
    "    gage_da = ds.gages[gage_mask].values.astype(int)\n",
    "\n",
    "    data_var_dict = {}\n",
    "    data_vars = (\"link\", \"to\", \"ascendingIndex\")\n",
    "    for v in data_vars:\n",
    "        data_var_dict[v] = ([\"gages\"], ds[v].values[gage_mask])\n",
    "    ds = xr.Dataset(data_vars=data_var_dict, coords={\"gages\": gage_da})\n",
    "df = ds.to_dataframe()\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usgs_df = df.join(df2)\n",
    "usgs_df = usgs_df.reset_index()\n",
    "usgs_df = usgs_df.rename(columns={\"index\": \"gages\"})\n",
    "usgs_df = usgs_df.set_index(\"link\")\n",
    "usgs_df = usgs_df.drop([\"gages\", \"ascendingIndex\", \"to\"], axis=1)\n",
    "usgs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_list = usgs_df.columns\n",
    "# columns_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, (len(columns_list) * 3) - 12, 12):\n",
    "    original_string = usgs_df.columns[i]\n",
    "    original_string_shortened = original_string[:-5]\n",
    "    temp_name1 = original_string_shortened + str(\"05:00\")\n",
    "    temp_name2 = original_string_shortened + str(\"10:00\")\n",
    "    temp_name3 = original_string_shortened + str(\"20:00\")\n",
    "    temp_name4 = original_string_shortened + str(\"25:00\")\n",
    "    temp_name5 = original_string_shortened + str(\"35:00\")\n",
    "    temp_name6 = original_string_shortened + str(\"40:00\")\n",
    "    temp_name7 = original_string_shortened + str(\"50:00\")\n",
    "    temp_name8 = original_string_shortened + str(\"55:00\")\n",
    "    usgs_df.insert(i + 1, temp_name1, np.nan)\n",
    "    usgs_df.insert(i + 2, temp_name2, np.nan)\n",
    "    usgs_df.insert(i + 4, temp_name3, np.nan)\n",
    "    usgs_df.insert(i + 5, temp_name4, np.nan)\n",
    "    usgs_df.insert(i + 7, temp_name5, np.nan)\n",
    "    usgs_df.insert(i + 8, temp_name6, np.nan)\n",
    "    usgs_df.insert(i + 10, temp_name7, np.nan)\n",
    "    usgs_df.insert(i + 11, temp_name8, np.nan)\n",
    "\n",
    "usgs_df = usgs_df.interpolate(method=\"linear\", axis=1)\n",
    "usgs_df\n",
    "# get_usgs_from_wrf_hydro(routelink_subset_file,usgs_timeslices_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from compose import compose\n",
    "\n",
    "\n",
    "def read_netcdfs(files, dim, transform_func=None):\n",
    "    def process_one_path(path):\n",
    "        with xr.open_dataset(path) as ds:\n",
    "            if transform_func is not None:\n",
    "                ds = transform_func(ds)\n",
    "            ds.load()\n",
    "            return ds\n",
    "\n",
    "    paths = sorted(glob.glob(files))\n",
    "    datasets = [process_one_path(p) for p in paths]\n",
    "    combined = xr.concat(datasets, dim)\n",
    "    return combined\n",
    "\n",
    "\n",
    "def preprocess_time_station_index(xd):\n",
    "    stationId_da_mask = list(\n",
    "        map(compose(bytes.isdigit, bytes.strip), xd.stationId.values)\n",
    "    )\n",
    "    stationId = xd.stationId[stationId_da_mask].values.astype(int)\n",
    "\n",
    "    unique_times_str = np.unique(xd.time.values).tolist()\n",
    "\n",
    "    unique_times = np.array(unique_times_str, dtype=\"str\")\n",
    "\n",
    "    data_var_dict = {}\n",
    "    data_vars = (\"discharge\", \"discharge_quality\")\n",
    "    for v in data_vars:\n",
    "        data_var_dict[v] = ([\"stationId\"], xd[v].values[stationId_da_mask])\n",
    "    return xr.Dataset(\n",
    "        data_vars=data_var_dict, coords={\"stationId\": stationId, \"time\": unique_times}\n",
    "    )\n",
    "\n",
    "\n",
    "# you might also use indexing operations like .sel to subset datasets\n",
    "# combined = read_netcdfs('/all/my/files/*.nc', dim='time',\n",
    "#                         transform_func=lambda ds: ds.mean())\n",
    "\n",
    "with read_netcdfs(\n",
    "    usgs_timeslices_folder + usgs_file_pattern_filter,\n",
    "    \"time\",\n",
    "    preprocess_time_station_index,\n",
    ") as ds2:\n",
    "    df2 = pd.DataFrame(\n",
    "        ds2[\"discharge\"].values.T,\n",
    "        index=ds2[\"stationId\"].values,\n",
    "        columns=ds2.time.values,\n",
    "    )\n",
    "\n",
    "with xr.open_dataset(routelink_subset_file) as ds:\n",
    "    gage_list = list(map(bytes.strip, ds.gages.values))\n",
    "    gage_mask = list(map(bytes.isdigit, gage_list))\n",
    "\n",
    "    gage_da = ds.gages[gage_mask].values.astype(int)\n",
    "\n",
    "    data_var_dict = {}\n",
    "    data_vars = (\"link\", \"to\", \"ascendingIndex\")\n",
    "    for v in data_vars:\n",
    "        data_var_dict[v] = ([\"gages\"], ds[v].values[gage_mask])\n",
    "    ds = xr.Dataset(data_vars=data_var_dict, coords={\"gages\": gage_da})\n",
    "df = ds.to_dataframe()\n",
    "\n",
    "usgs_df = df.join(df2)\n",
    "usgs_df = usgs_df.reset_index()\n",
    "usgs_df = usgs_df.rename(columns={\"index\": \"gages\"})\n",
    "usgs_df = usgs_df.set_index(\"link\")\n",
    "usgs_df = usgs_df.drop([\"gages\", \"ascendingIndex\", \"to\"], axis=1)\n",
    "columns_list = usgs_df.columns\n",
    "\n",
    "for i in range(0, (len(columns_list) * 3) - 12, 12):\n",
    "    original_string = usgs_df.columns[i]\n",
    "    original_string_shortened = original_string[:-5]\n",
    "    temp_name1 = original_string_shortened + str(\"05:00\")\n",
    "    temp_name2 = original_string_shortened + str(\"10:00\")\n",
    "    temp_name3 = original_string_shortened + str(\"20:00\")\n",
    "    temp_name4 = original_string_shortened + str(\"25:00\")\n",
    "    temp_name5 = original_string_shortened + str(\"35:00\")\n",
    "    temp_name6 = original_string_shortened + str(\"40:00\")\n",
    "    temp_name7 = original_string_shortened + str(\"50:00\")\n",
    "    temp_name8 = original_string_shortened + str(\"55:00\")\n",
    "    usgs_df.insert(i + 1, temp_name1, np.nan)\n",
    "    usgs_df.insert(i + 2, temp_name2, np.nan)\n",
    "    usgs_df.insert(i + 4, temp_name3, np.nan)\n",
    "    usgs_df.insert(i + 5, temp_name4, np.nan)\n",
    "    usgs_df.insert(i + 7, temp_name5, np.nan)\n",
    "    usgs_df.insert(i + 8, temp_name6, np.nan)\n",
    "    usgs_df.insert(i + 10, temp_name7, np.nan)\n",
    "    usgs_df.insert(i + 11, temp_name8, np.nan)\n",
    "\n",
    "usgs_df = usgs_df.interpolate(method=\"linear\", axis=1)\n",
    "usgs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": ""
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
