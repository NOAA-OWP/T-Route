{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusive routing applied to a basin above Falls Lake, NC using\n",
    "   1. Crank Nicolson Hermite Interpolation derived by Prof.Ehab Meselhe, Tulane University and  \n",
    "   coded by Md Nazmul Azim Beg, Postdoc of Prof.Ehab Meselhe.\n",
    "   2. Fixed time step with internally/uniformly adjusting segment length inside of Fortran to    meet Courant condition, developed by Dong Ha Kim, NOAA's OWP, National Water Center\n",
    "   3. Computation over entire time step and over entire network is done inside of Fortran while this Python routing framework v01 computes normal depth lookup table, irregular channel geometry lookup table (for this application, this table is prepared but not used inside Fotran), and Fortran-Python network traversal map.  The framework also pass channel geometry information, qlatral and boundary condition time series, and simulation time parameters to Fortran.\n",
    "   \n",
    "### Fortran source code: \n",
    "   1. nrtype.f90\n",
    "   2. var_module.f90\n",
    "   3. subtools.f90\n",
    "   4. readXsection.f90\n",
    "   5. uniformflowLT.f90\n",
    "   6. mod_dx_tools.f90\n",
    "   7. ef_q_y_calc.f90\n",
    "   8. maindiff.f90\n",
    "   \n",
    "### Python py files:\n",
    "   1. xsec_lookuptable_F.py\n",
    "   2. routelink_adjustment.py\n",
    "   3. waterdepth_lookuptable_F.py\n",
    "   4. fortran_python_mapping.py\n",
    "   5. set_boundary_F2.py\n",
    "   6. qlateral_retrieval_F.py\n",
    "   \n",
    "### f2pyc wrapped library:\n",
    "   dfcnhi_mt.cpython-37m-x86_64-linux-gnu.so\n",
    "   \n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating supernetwork connections set\n",
      "supernetwork connections set complete\n",
      "... in 0.22204995155334473 seconds.\n",
      "organizing connections into networks and reaches ...\n",
      "reach organization complete\n",
      "... in 0.0054857730865478516 seconds.\n",
      "executing serial computation on ordered reaches ...\n",
      "terminal_segment iter-> 1\n",
      "933020089 completed\n",
      "... in 2.842141628265381 seconds.\n",
      "creating qlateral array ...\n",
      "qlateral array complete\n",
      "... in 0.7913336753845215 seconds.\n",
      "nl_ubcd_ar_g:7068\n",
      "ordered reach computation complete\n",
      "... in 5.860745668411255 seconds.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "#import globalvar as g\n",
    "import dfcnhi_mt as df  \n",
    "\n",
    "root=None\n",
    "\n",
    "verbose = True\n",
    "debuglevel = 0\n",
    "showtiming = True\n",
    "start_time= None\n",
    "\n",
    "connections= None\n",
    "supernetwork_data=None\n",
    "network= None\n",
    "ordered_reaches= None\n",
    "seg_list_all= None\n",
    "ncompall= None\n",
    "fksegID_dsend= None\n",
    "                   \n",
    "dbsegID= None\n",
    "dbfksegID= None\n",
    "\n",
    "nel_g= None\n",
    "xsec_attr_rch_g= None\n",
    "nhincr_m_g= None\n",
    "nhincr_f_g= None\n",
    "ufqlt_m_g= None                       \n",
    "ufhlt_m_g= None  \n",
    "ufqlt_f_g= None  \n",
    "ufhlt_f_g= None  \n",
    "z_all= None\n",
    "\n",
    "#pynw= None\n",
    "#frnw_g= None\n",
    "z_ar_g= None\n",
    "bo_ar_g= None\n",
    "traps_ar_g= None\n",
    "tw_ar_g= None\n",
    "twcc_ar_g= None\n",
    "mann_ar_g = None\n",
    "manncc_ar_g= None\n",
    "so_ar_g=None\n",
    "dx_ar_g= None \n",
    "\n",
    "mxncomp_g=None\n",
    "nrch_g=None\n",
    "\n",
    "pynw= None\n",
    "frnw_g= None\n",
    "\n",
    "output_path='./output2'\n",
    "\n",
    "##-----------------------------------------------------------------------------------\n",
    "#    Set channel geometry data, xsec_tables, normal depth tables, and \n",
    "#    Fortran to Python Mapping tables\n",
    "#\n",
    "##-----------------------------------------------------------------------------------\n",
    "def setting_hydTables_Fr_PyMap():\n",
    "    \n",
    "    global root\n",
    "    global start_time \n",
    "    \n",
    "    global connections\n",
    "    global supernetwork_data\n",
    "    global network\n",
    "    global ordered_reaches\n",
    "    global seg_list_all\n",
    "    global ncompall\n",
    "    global fksegID_dsend\n",
    "        \n",
    "    global dbsegID\n",
    "    global dbfksegID\n",
    "\n",
    "    global nel_g\n",
    "    global xsec_attr_rch_g\n",
    "    global nhincr_m_g\n",
    "    global nhincr_f_g\n",
    "    global ufqlt_m_g                       \n",
    "    global ufhlt_m_g \n",
    "    global ufqlt_f_g \n",
    "    global ufhlt_f_g \n",
    "    \n",
    "    global z_all\n",
    "    global z_ar_g\n",
    "    global bo_ar_g\n",
    "    global traps_ar_g\n",
    "    global tw_ar_g\n",
    "    global twcc_ar_g\n",
    "    global mann_ar_g\n",
    "    global manncc_ar_g\n",
    "    global so_ar_g\n",
    "    global dx_ar_g\n",
    "    \n",
    "    global mxncomp_g\n",
    "    global nrch_g\n",
    "    \n",
    "    global pynw\n",
    "    global frnw_g\n",
    "\n",
    "\n",
    "    from sys import platform\n",
    "    if platform == \"linux\" or platform == \"linux2\":\n",
    "        pass\n",
    "    elif platform == \"darwin\":\n",
    "        pass\n",
    "    elif platform == \"win32\":\n",
    "        print('The parallel version of compute_nhd_routing.py will not execute as currently')\n",
    "        print('written due to the lack of a fork() capability in the windows OS.')\n",
    "        print('For parallel execution, please us a *nix OS.')\n",
    "        print('\\nexiting...')\n",
    "        sys.exit()\n",
    "        # Some useful references:\n",
    "        # https://stackoverflow.com/questions/985281/what-is-the-closest-thing-windows-has-to-fork/985525#985525\n",
    "        # https://stackoverflow.com/questions/8220108/how-do-i-check-the-operating-system-in-python\n",
    "        # https://stackoverflow.com/questions/6596617/python-multiprocess-diff-between-windows-and-linux\n",
    "    ENV_IS_CL = False\n",
    "    if ENV_IS_CL: root = '/content/t-route/'\n",
    "    elif not ENV_IS_CL: \n",
    "        sys.setrecursionlimit(4000)\n",
    "    #     root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n",
    "        root = os.path.dirname(os.path.dirname(os.path.abspath('')))\n",
    "        sys.path.append(os.path.join(root, r'src', r'python_framework'))\n",
    "        sys.path.append(os.path.join(root, r'src', r'python_routing')) #by Dong Ha\n",
    "        sys.path.append(os.path.join(root, r'src', r'evaluation',r'_restclient')) # for usgs data retrieval\n",
    "        sys.path.append(os.path.join(root, r'src', r'evaluation',r'nwis_client')) # for usgs data retrieval\n",
    "        fortran_source_dir = os.path.join(root, r'src', r'fortran_routing', r'diffCNHI_pylink')\n",
    "        sys.path.append(fortran_source_dir)\n",
    "\n",
    "    ## F2PY fortran source codes of diffusive routing using Crank-Nicolson Hermite Interpolation \n",
    "    # mc_f2py= True\n",
    "    # if mc_f2py:\n",
    "    #     try:\n",
    "    #         import subprocess\n",
    "\n",
    "    #         f2py_call = []\n",
    "    #         f2py_call.append(r'f2py3')\n",
    "    #         f2py_call.append(r'-c')\n",
    "    #         f2py_call.append(r'nrtype.f90')\n",
    "    #         f2py_call.append(r'var_module.f90')\n",
    "    #         f2py_call.append(r'subtools.f90')\n",
    "    #         f2py_call.append(r'readXsection.f90')\n",
    "    #         f2py_call.append(r'uniformflowLT.f90')\n",
    "    #         f2py_call.append(r'calculateDT.f90')\n",
    "    #         f2py_call.append(r'ef_q_y_calc.f90')\n",
    "    #         f2py_call.append(r'-m')\n",
    "    #         f2py_call.append(r'dfcnhi')\n",
    "    #         subprocess.run(f2py_call, cwd=r'../fortran_routing/diffCNHI_pylink',\n",
    "    #                        stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "    #     except Exception as e:\n",
    "    #         print(e)\n",
    "\n",
    "    #** network and reach utilities **\n",
    "    import nhd_network_utilities as nnu\n",
    "    import nhd_reach_utilities as nru\n",
    "    import numpy as np\n",
    "    import statistics as stat\n",
    "    import math\n",
    "    #import pixiedust\n",
    "    import xsec_lookuptable_F as xseclt\n",
    "    import routelink_adjustment as rladj\n",
    "    import waterdepth_lookuptable_F as wdlt\n",
    "    import fortran_python_mapping as fpm\n",
    "\n",
    "    #global connections\n",
    "    #global geo_input_folder\n",
    "\n",
    "    test_folder = os.path.join(root, r'test')\n",
    "    geo_input_folder = os.path.join(test_folder, r'input', r'geo', r'Channels')\n",
    "    # mainly for retrieving qlateral\n",
    "    custom_input_folder = os.path.join(root, r'test', r'input', 'yaml') \n",
    "    custom_input_file = \"florence_933020089_dt300.yaml\"\n",
    "    custom_input_file_path=os.path.join(custom_input_folder,custom_input_file)  \n",
    "\n",
    "\n",
    "    #TODO: Make these commandline args\n",
    "    #supernetwork= 'Pocono_TEST3_3R1J'\n",
    "    #supernetwork='Pocono_TEST2'  # 29 reaches for MC parity test\n",
    "    supernetwork='Florence_933020089'\n",
    "    \"\"\"##NHD Subset (Brazos/Lower Colorado)\"\"\"\n",
    "    #     supernetwork = 'Brazos_LowerColorado_ge5'\n",
    "    \"\"\"##NHD CONUS order 5 and greater\"\"\"\n",
    "    # supernetwork = 'CONUS_ge5'\n",
    "    \"\"\"These are large -- be careful\"\"\"                          \n",
    "    #     supernetwork = 'Mainstems_CONUS'\n",
    "    # supernetwork = 'CONUS_FULL_RES_v20'\n",
    "    # supernetwork = 'CONUS_Named_Streams' #create a subset of the full resolution by reading the GNIS field\n",
    "    # supernetwork = 'CONUS_Named_combined' #process the Named streams through the Full-Res paths to join the many hanging reaches\n",
    "\n",
    "    if verbose: print('creating supernetwork connections set')\n",
    "    if showtiming: start_time = time.time()\n",
    "    #STEP 1\n",
    "    supernetwork_data, supernetwork_values = nnu.set_networks(\n",
    "        supernetwork = supernetwork\n",
    "        , geo_input_folder = geo_input_folder\n",
    "        , verbose = False\n",
    "        # , verbose = verbose\n",
    "        , debuglevel = debuglevel\n",
    "        )\n",
    "    if verbose: print('supernetwork connections set complete')\n",
    "    if showtiming: print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "    #STEP 2\n",
    "    if showtiming: start_time = time.time()\n",
    "    if verbose: print('organizing connections into networks and reaches ...')\n",
    "    networks = nru.compose_reaches(\n",
    "        supernetwork_values\n",
    "        , verbose = False\n",
    "        # , verbose = verbose\n",
    "        , debuglevel = debuglevel\n",
    "        , showtiming = showtiming\n",
    "        )\n",
    "    if verbose: print('reach organization complete')\n",
    "    if showtiming: print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "    #STEP 3: Route NHD streamflow\n",
    "    if showtiming: start_time = time.time()\n",
    "    executiontype = 'serial' # 'parallel'\n",
    "\n",
    "    if verbose: print('executing serial computation on ordered reaches ...')\n",
    "    connections = supernetwork_values[0]\n",
    "\n",
    "\n",
    "    #----------------------------------------------------------\n",
    "    # x-sec attribute lookup table variables\n",
    "\n",
    "    #----------------------------------------------------------    \n",
    "    nxsecpt_g =8 #the number of (x,y) points for x-sec geometry\n",
    "    nel_g=21 # the number of incrementally increasing water depth lines\n",
    "    timesdepth_g=5.0  # multiplier to bankfull depth to cover potentially largest water depth\n",
    "    #-------------------------------------------------------------\n",
    "    #  uniform flow discharge-normal depth lookup table variables\n",
    "\n",
    "    #-------------------------------------------------------------     \n",
    "    nhincr_m_g=5  # the number of depth increments of main channel\n",
    "    nhincr_f_g=10 # the number of depth increments of floodplain\n",
    "    #-------------------------------------------------------------\n",
    "    #  downstream boundary parameters\n",
    "\n",
    "    #-------------------------------------------------------------     \n",
    "    #dbsegID= 4185661  # downstream boundary segment ID for supernetwork= 'Pocono_TEST3_3R1J'\n",
    "    #dbsegID= 4186169  # downstream boundary segment ID for supernetwork='Pocono_TEST2' \n",
    "    dbsegID= 933020089  # downstream boundary segment ID for supernetwork='Florence_933020089' \n",
    "    #dbseg_usgsID='02087182' #usgs site ID corresponding to dbsegID for downstream boundary condition\n",
    "    \n",
    "    #dbsegID: downstream boundary segment ID -> make a fake segment\n",
    "    dbfksegID= str(dbsegID) + str(2)\n",
    "    dbfksegID= int(dbfksegID) \n",
    "\n",
    "    iter=0\n",
    "    #for terminal_segment, network in networks.items():     \n",
    "    if showtiming: \n",
    "        network_start_time = time.time()\n",
    "    iter=iter+1         \n",
    "    #print(f\"terminal_segment:{terminal_segment} network:{network}\")\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #toseg=connections[terminal_segment]['data'][supernetwork_data['downstream_col']]\n",
    "    #if toseg != 0 or terminal_segment==dbsegID:\n",
    "\n",
    "    terminal_segment= dbsegID\n",
    "    network= networks[terminal_segment]\n",
    "    #print(f\"terminal_segment{terminal_segment} toseg:{toseg}\")    \n",
    "    print(f\"terminal_segment iter-> {iter}\")   \n",
    "\n",
    "    # ** x-sec attribute lookup table variables **    \n",
    "    # nel: the number of incrementally increasing water depth lines\n",
    "    xsec_attr_all={connection:{'elev':np.zeros(nel_g)\n",
    "                        ,'convey':np.zeros(nel_g)\n",
    "                        ,'topwd':np.zeros(nel_g)\n",
    "                        ,'uniflow':np.zeros(nel_g)}\n",
    "                           for connection in connections} \n",
    "    # ** uniform flow discharge-normal depth lookup table variables **\n",
    "    # nhincr_m_g: the number of depth increments of main channel\n",
    "    # nhincr_f_g: the number of depth increments of floodplain\n",
    "    uflt={connection:{'q_m':np.zeros(nhincr_m_g)\n",
    "                      ,'h_m':np.zeros(nhincr_m_g)\n",
    "                      ,'q_f':np.zeros(nhincr_f_g)\n",
    "                      ,'h_f':np.zeros(nhincr_f_g)}\n",
    "                        for connection in connections} \n",
    "    # ** channel attribute adjustment **\n",
    "    z_all={connection:{'adj.alt':np.zeros(1)}\n",
    "                        for connection in connections} \n",
    "    # dictionary all keyed by head_segment key\n",
    "    ordered_reaches = {}\n",
    "    last_segment_reach={}\n",
    "    lsegs_usrch={}\n",
    "    ncompall={}\n",
    "    fksegID_dsend={}\n",
    "    seg_list_all={}\n",
    "    hseg_list=[]\n",
    "    rchlen_all={}\n",
    "\n",
    "    for head_segment, reach in network['reaches'].items():\n",
    "        if reach['seqorder'] not in ordered_reaches:\n",
    "            ordered_reaches.update({reach['seqorder']:[]}) \n",
    "        # Ordered_reaches contains network dictionary values according to seqorder (= \n",
    "        # the number of juctions downstream).\n",
    "        ordered_reaches[reach['seqorder']].append([head_segment, reach])\n",
    "        last_segment_reach[head_segment]= reach['reach_tail']\n",
    "        hseg_list.append(head_segment)\n",
    "\n",
    "    # Count the total number of segments for each reach in computation\n",
    "    # ** Note that routing models use node configuration where one segment has\n",
    "    # ** has upper and lower nodes and each segment shares common nodes at either \n",
    "    # ** direction. So, the number of nodes of a given reaach becomes equiv. to\n",
    "    # ** the number of segments + 1.\n",
    "    tnseg=0\n",
    "    nrch_g=0 # the number of reaches\n",
    "    mxncomp_g=0 # max. number of segments within a reach\n",
    "    for x in range(network['maximum_reach_seqorder'],-1,-1):  \n",
    "        for head_segment, reach in ordered_reaches[x]:                        \n",
    "            seg_list0=list(reach['segments_list'])\n",
    "            seg_list=seg_list0[::-1] #to reversed order\n",
    "            # check if there is a downstream boundary segment in the given reach.\n",
    "            if seg_list0.count(dbsegID)>0:\n",
    "                # identify where dsbdID is located along the given reach, and count \n",
    "                # segments upward from and including the dsbdID segment.                    \n",
    "                idx= seg_list0.index(dbsegID)\n",
    "                tnseg= tnseg+len(seg_list)-idx\n",
    "                ncomp= len(seg_list)-idx+1 \n",
    "            else:\n",
    "                #the number of segments of a given reach \n",
    "                tnseg= tnseg+ len(seg_list)\n",
    "                ncomp= len(seg_list)+1\n",
    "            ncompall.update({head_segment:ncomp})\n",
    "            if ncomp>mxncomp_g:\n",
    "                mxncomp_g= ncomp\n",
    "            nrch_g= nrch_g +1 # counts the number of reaches \n",
    "            # store reach length of each link for evalution later\n",
    "            for seg in range(0,ncomp-1):                    \n",
    "                segID= seg_list[seg-1]                \n",
    "                rchlen=connections[segID]['data'][supernetwork_data['length_col']]\n",
    "                rchlen_all.update({segID:rchlen})\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    #                            Step 0-2    \n",
    "\n",
    "    #  Update key dictionary variables\n",
    "    #---------------------------------------------------------------------------------------\n",
    "    # Add one more fake segment after downstream terminal segment in a reach\n",
    "    # by \"terminal segmentID\"+\"2\". This is to account for one more node in\n",
    "    # Fortran node configuration, which is equivalent to Python segment \n",
    "    # configuration.\n",
    "            fksegID= seg_list[ncomp-2]\n",
    "            fksegID= str(fksegID) + str(2)\n",
    "            fksegID=int(fksegID)\n",
    "            fksegID_dsend.update({head_segment:fksegID})                     \n",
    "\n",
    "            xsec_attr_all.update({fksegID:{'elev':np.zeros(nel_g)\n",
    "                                        ,'convey':np.zeros(nel_g)\n",
    "                                        ,'topwd':np.zeros(nel_g)\n",
    "                                        ,'uniflow':np.zeros(nel_g)}})\n",
    "\n",
    "            uflt.update({fksegID:{'q_m':np.zeros(nhincr_m_g)\n",
    "                                  ,'h_m':np.zeros(nhincr_m_g)\n",
    "                                  ,'q_f':np.zeros(nhincr_f_g)\n",
    "                                  ,'h_f':np.zeros(nhincr_f_g)}})\n",
    "\n",
    "            seg_list.append(fksegID_dsend[head_segment])\n",
    "\n",
    "            seg_list_all.update({head_segment:seg_list})\n",
    "\n",
    "            z_all.update({fksegID:{'adj.alt':np.zeros(1)}}) \n",
    "\n",
    "    cel_av_g=np.zeros(nrch_g) # holds reach-averaged celerities\n",
    "    #import pdb; pdb.set_trace()\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    #                                 Step 0-3           \n",
    "\n",
    "    #    Adjust altitude so that altitude of the last sement of a reach is equal to that \n",
    "    #    of the first segment of its downstream reach right after their common junction.\n",
    "    #--------------------------------------------------------------------------------------\n",
    "    rladj.adj_alt1(        \n",
    "                    connections= connections \n",
    "                    , supernetwork_data= supernetwork_data \n",
    "                    , network= network\n",
    "                    , ordered_reaches= ordered_reaches\n",
    "                    , seg_list_all= seg_list_all\n",
    "                    , ncompall= ncompall \n",
    "                    , dbfksegID= dbfksegID\n",
    "                    , z_all= z_all\n",
    "                    ) \n",
    "    #--------------------------------------------------------------------------------------\n",
    "    #                                 Step 0-4           \n",
    "\n",
    "    #     Make Fortran-Python channel network mapping variables.\n",
    "    #--------------------------------------------------------------------------------------   \n",
    "    pynw={}\n",
    "    frj=-1\n",
    "    for x in range(network['maximum_reach_seqorder'],-1,-1):       \n",
    "        for head_segment, reach in ordered_reaches[x]:\n",
    "            frj= frj+1\n",
    "            pynw[frj]=head_segment        \n",
    "    frnw_g=np.zeros((nrch_g,8), dtype=int)\n",
    "    \n",
    "    fpm.fpmap1(\n",
    "            connections= connections \n",
    "            , supernetwork_data= supernetwork_data \n",
    "            , network= network\n",
    "            , ordered_reaches= ordered_reaches\n",
    "            , seg_list_all= seg_list_all\n",
    "            , ncompall= ncompall\n",
    "            , nrch_g= nrch_g\n",
    "            , dbfksegID= dbfksegID\n",
    "            , z_all= z_all\n",
    "            , pynw= pynw\n",
    "            , frnw_g= frnw_g\n",
    "            )  \n",
    "    \n",
    "    pynwf=np.zeros((nrch_g,2), dtype=int)\n",
    "    frj=-1\n",
    "    for x in range(network['maximum_reach_seqorder'],-1,-1):       \n",
    "        for head_segment, reach in ordered_reaches[x]:\n",
    "            frj= frj+1\n",
    "            pynwf[frj,0]= frj+1 # +1 is added to accommodate Fortran indexing\n",
    "            pynwf[frj,1]=head_segment\n",
    "            \n",
    "    np.savetxt(\"./output2/frnw_ar.txt\", frnw_g, fmt='%10i')\n",
    "    np.savetxt(\"./output2/pynw_ar.txt\", pynwf, fmt='%20i')\n",
    "    #---------------------------------------------------------------------------------\n",
    "    #                              Step 0-5\n",
    "\n",
    "    #                  Prepare channel geometry data           \n",
    "    #---------------------------------------------------------------------------------    \n",
    "    z_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    bo_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    traps_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    tw_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    twcc_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    mann_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    manncc_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    so_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    dx_ar_g=np.zeros((mxncomp_g, nrch_g))\n",
    "    frj=-1\n",
    "    for x in range(network['maximum_reach_seqorder'],-1,-1):   \n",
    "        for head_segment, reach in ordered_reaches[x]:                  \n",
    "            seg_list= seg_list_all[head_segment] \n",
    "            ncomp=ncompall[head_segment]              \n",
    "            frj=frj+1\n",
    "            for seg in range(0,ncomp):                    \n",
    "                if seg==ncomp-1:\n",
    "                    segID= seg_list[seg-1]\n",
    "                else:\n",
    "                    segID= seg_list[seg]                               \n",
    "\n",
    "                bo_ar_g[seg, frj]=connections[segID]['data'][supernetwork_data['bottomwidth_col']]                \n",
    "                traps=connections[segID]['data'][supernetwork_data['ChSlp_col']]\n",
    "                traps_ar_g[seg, frj]=1.0/traps  # s in 1 over s for channel side slope               \n",
    "                tw_ar_g[seg, frj]=connections[segID]['data'][supernetwork_data['topwidth_col']]\n",
    "                twcc_ar_g[seg, frj]=connections[segID]['data'][supernetwork_data['topwidthcc_col']]\n",
    "                mann_ar_g[seg, frj]=connections[segID]['data'][supernetwork_data['manningn_col']]\n",
    "                manncc_ar_g[seg, frj]=connections[segID]['data'][supernetwork_data['manningncc_col']]\n",
    "                so_ar_g[seg, frj]=connections[segID]['data'][supernetwork_data['slope_col']]\n",
    "                dx_ar_g[seg, frj]=connections[segID]['data'][supernetwork_data['length_col']]\n",
    "                segID1= seg_list[seg]  \n",
    "                z_ar_g[seg, frj]= z_all[segID1]['adj.alt'][0]    \n",
    "                \n",
    "             \n",
    "    # saved txt file has row for segment and colunum for frj\n",
    "    np.savetxt(\"./output2/z_ar.txt\", z_ar_g, fmt='%15.5f')\n",
    "    np.savetxt(\"./output2/bo_ar.txt\", bo_ar_g, fmt='%15.5f')\n",
    "    np.savetxt(\"./output2/traps_ar.txt\", traps_ar_g, fmt='%15.5f')\n",
    "    np.savetxt(\"./output2/tw_ar.txt\", tw_ar_g, fmt='%15.5f')\n",
    "    np.savetxt(\"./output2/twcc_ar.txt\", twcc_ar_g, fmt='%15.5f')\n",
    "    np.savetxt(\"./output2/mann_ar.txt\", mann_ar_g, fmt='%15.5f')\n",
    "    np.savetxt(\"./output2/manncc_ar.txt\", manncc_ar_g, fmt='%15.5f')\n",
    "    np.savetxt(\"./output2/so_ar.txt\", so_ar_g, fmt='%15.6f') \n",
    "    np.savetxt(\"./output2/dx_ar.txt\", dx_ar_g, fmt='%15.5f')\n",
    "    \n",
    "    \n",
    "    #---------------------------------------------------------------------------------------\n",
    "    #                                 Step 0-6\n",
    "\n",
    "    #  Make channel x-sec attribute lookup tables for each node (p.132,rm4)\n",
    "    #  ** tzeq_flag_g= 0: use an approximation method based on finite rectangles or triangle \n",
    "    #                       for any regular or irregular x-sec\n",
    "    #                1: use related equations only for trapezoidal main & rectangular \n",
    "    #                        floodplains\n",
    "    #  ** Output: \n",
    "    #            xsec_attr_all\n",
    "    #              ** this variable also stores uniform flow values corresponding to elev.\n",
    "    #----------------------------------------------------------------------------------------\n",
    "    xseclt.xsecTR_ltable(        \n",
    "                        connections= connections\n",
    "                        , supernetwork_data= supernetwork_data\n",
    "                        , network= network\n",
    "                        , ordered_reaches= ordered_reaches\n",
    "                        , seg_list_all= seg_list_all\n",
    "                        , ncompall= ncompall\n",
    "                        , z_all= z_all\n",
    "                        , nxsecpt_g= nxsecpt_g\n",
    "                        , nel_g= nel_g\n",
    "                        , timesdepth_g= timesdepth_g\n",
    "                        , tzeq_flag_g= 1\n",
    "                        , xsec_attr_all= xsec_attr_all\n",
    "                        )\n",
    "\n",
    "    xsec_attr_rch_g=np.zeros((mxncomp_g, nrch_g, nel_g, 4))\n",
    "    frj=-1\n",
    "    #with open(os.path.join(output_path,\"Python_xsec_attr_rch_g\"),'a') as xarch:\n",
    "    for x in range(network['maximum_reach_seqorder'],-1,-1):       \n",
    "        for head_segment, reach in ordered_reaches[x]:           \n",
    "            seg_list= seg_list_all[head_segment] \n",
    "            ncomp=ncompall[head_segment]   \n",
    "            frj=frj+1\n",
    "            for seg in range(0,ncomp):                  \n",
    "                segID= seg_list[seg]\n",
    "                for i1 in range(0,nel_g):\n",
    "                    xsec_attr_rch_g[seg, frj, i1,0]= xsec_attr_all[segID]['elev'][i1]\n",
    "                    xsec_attr_rch_g[seg, frj, i1,1]= xsec_attr_all[segID]['convey'][i1]\n",
    "                    xsec_attr_rch_g[seg, frj, i1,2]= xsec_attr_all[segID]['topwd'][i1]\n",
    "                    xsec_attr_rch_g[seg, frj, i1,3]= xsec_attr_all[segID]['uniflow'][i1]\n",
    "                    #if i1==0:\n",
    "                    #    print(f\"xsec_attr_rch_g seg:{seg} frj:{frj} segID:{segID}\")\n",
    "                    #xarch.write(\"%s %s %s %s %s %s %s\\n\" %\\\n",
    "                    #           (seg, frj, segID,\\\n",
    "                    #            xsec_attr_rch_g[seg,frj,i1,0],\\\n",
    "                    #            xsec_attr_rch_g[seg,frj,i1,1],\\\n",
    "                    #            xsec_attr_rch_g[seg,frj,i1,2],\\\n",
    "                    #            xsec_attr_rch_g[seg,frj,i1,3],\\\n",
    "                    #            ))\n",
    "    #---------------------------------------------------------------------------------\n",
    "    #                              Step 0-7\n",
    "\n",
    "    #  Create uniform flow-normal depth lookup tables using channel geometry equations\n",
    "    #  ONLY for trapezoidal main and rectangular floodplains (p.132,rm4)            \n",
    "    #---------------------------------------------------------------------------------\n",
    "    ufqlt_m_g= np.zeros((mxncomp_g,nrch_g,nhincr_m_g))                        \n",
    "    ufhlt_m_g= np.zeros((mxncomp_g,nrch_g,nhincr_m_g))\n",
    "    ufqlt_f_g= np.zeros((mxncomp_g,nrch_g,nhincr_f_g))\n",
    "    ufhlt_f_g= np.zeros((mxncomp_g,nrch_g,nhincr_f_g))\n",
    "    \n",
    "    wdlt.ufTR_ltable_frnw(        \n",
    "                    connections= connections \n",
    "                    , supernetwork_data= supernetwork_data\n",
    "                    , network= network\n",
    "                    , ordered_reaches= ordered_reaches\n",
    "                    , seg_list_all= seg_list_all\n",
    "                    , ncompall= ncompall\n",
    "                    , mxncomp_g= mxncomp_g\n",
    "                    , nrch_g= nrch_g\n",
    "                    , frnw_g= frnw_g\n",
    "                    , timesdepth_g= timesdepth_g\n",
    "                    , nhincr_m_g= nhincr_m_g\n",
    "                    , nhincr_f_g= nhincr_f_g\n",
    "                    , ufqlt_m_g= ufqlt_m_g\n",
    "                    , ufhlt_m_g= ufhlt_m_g\n",
    "                    , ufqlt_f_g= ufqlt_f_g\n",
    "                    , ufhlt_f_g= ufhlt_f_g\n",
    "                    , bo_ar_g=bo_ar_g\n",
    "                    , traps_ar_g= traps_ar_g\n",
    "                    , tw_ar_g=tw_ar_g\n",
    "                    , twcc_ar_g=twcc_ar_g\n",
    "                    , mann_ar_g= mann_ar_g\n",
    "                    , manncc_ar_g= manncc_ar_g\n",
    "                    , so_ar_g= so_ar_g      \n",
    "                    )       \n",
    "    \n",
    "#    frj=-1\n",
    "    #with open(os.path.join(output_path,\"Python_uftable_f\"),'a') as uftb:\n",
    "#    for x in range(network['maximum_reach_seqorder'],-1,-1): \n",
    "#        for head_segment, reach in ordered_reaches[x]:                  \n",
    "#            seg_list= seg_list_all[head_segment] \n",
    "#            ncomp=ncompall[head_segment]\n",
    "#            frj=frj+1\n",
    "#            for seg in range(0,ncomp):                  \n",
    "#                segID= seg_list[seg]\n",
    "#                for i1 in range(0,nhincr_m_g):\n",
    "#                    ufqlt_m_g[seg, frj, i1]=uflt[segID]['q_m'][i1]\n",
    "#                    ufhlt_m_g[seg, frj, i1]=uflt[segID]['h_m'][i1]\n",
    "#                for i1 in range(0,nhincr_f_g):\n",
    "#                    ufqlt_f_g[seg, frj, i1]=uflt[segID]['q_f'][i1] \n",
    "#                    ufhlt_f_g[seg, frj, i1]=uflt[segID]['h_f'][i1]\n",
    "                    #uftb.write(\"%s %s %s %s %s\\n\" %\\\n",
    "                    #        (i1, frj, segID,\\\n",
    "                    #        ufqlt_f_g[seg,frj,i1], ufhlt_f_g[seg,frj,i1]))                        \n",
    "    #print(f\"uflt inside:{uflt}\")\n",
    "  \n",
    "                            \n",
    "    if verbose: print(f'{terminal_segment} completed')\n",
    "    if showtiming: print(\"... in %s seconds.\" % (time.time() - network_start_time))\n",
    "\n",
    "    #if verbose: print('ordered reach computation complete')\n",
    "    #if showtiming: print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "        \n",
    "       \n",
    "    \n",
    "def set_run_diff_routing():\n",
    "  \n",
    "    import set_boundarydata_F2 as setbd\n",
    "    import qlateral_retrieval_F as qlrtv\n",
    "    \n",
    "    # mainly for retrieving qlateral\n",
    "    custom_input_folder = os.path.join(root, r'test', r'input', 'yaml') \n",
    "    custom_input_file = \"florence_933020089_dt300.yaml\"\n",
    "    custom_input_file_path=os.path.join(custom_input_folder,custom_input_file)  \n",
    "    output_path='./output2'\n",
    "    \n",
    "    #------------------------------------------------------\n",
    "    #        diffusive CNHI model time variables\n",
    "    #------------------------------------------------------  \n",
    "    dtini_g= 300.0   # initial time interval in [sec]\n",
    "    t0_g= 0.0        # simulation starting time in [hr]\n",
    "    tfin_g= 18.0      # simulation ending time in [hr] \n",
    "    cfl_g= 0.99      # max. allowable Courant number \n",
    "    dt_ql_g= 3600.0  # instead of dtinput #time interval of qlateral input data from wrf-hydro [sec]\n",
    "    dt_ub_g= dt_ql_g # time interval of input data for upstream boundary condition (= headbasin seg lat.flow)  [sec]\n",
    "    dt_db_g=  900.0  # time interval of input data for downstream boundary condition  [sec]\n",
    "    saveinterval_ev_g= dtini_g # for evaluation later [sec]\n",
    "    usgssDT='2018-08-01'       # start date of USGS retrieval tool\n",
    "    usgseDT='2018-08-02'       # end date of USGS retrieval tool\n",
    "    dbseg_usgsID='02087182'    # usgs site ID corresponding to dbsegID for downstream boundary condition\n",
    "    \n",
    "    ntss_ev_g= int((tfin_g - t0_g)*3600.0/saveinterval_ev_g)+1\n",
    "    #ydaq_ev = {connection:{'elev':np.zeros(ntss_ev_g)\n",
    "    #                        ,'q':np.zeros(ntss_ev_g)\n",
    "    #                        ,'celerity':np.zeros(ntss_ev_g)\n",
    "    #                        ,'diffusivity':np.zeros(ntss_ev_g)}\n",
    "    #                        for connection in connections}\n",
    "    # ** lateral inflows **\n",
    "    # qlateral time variables\n",
    "    nts_ql_g= (tfin_g - t0_g)*3600.0/dt_ql_g+1 # based on time of [sec]\n",
    "    nts_ql_g= int(nts_ql_g)\n",
    "    qlatral = {connection:{'qlat':np.zeros(nts_ql_g)\n",
    "                        } for connection in connections}            \n",
    "    # ** upstream boundary condition ** \n",
    "    # For Florence case, headbasin segment discharge=0 m^3/sec.\n",
    "    nts_ub_g= nts_ql_g \n",
    "    ubcond={connection:{'elev':np.zeros(nts_ub_g)\n",
    "                        , 'discharge':np.zeros(nts_ub_g)\n",
    "                        } for (connection, cv) in connections.items()\n",
    "                                                 if cv['upstreams']=={0}}\n",
    "    # ** downstream boundary condition **\n",
    "    # For Florence case, measured Fall lake elevation is used.\n",
    "    nts_db_g= (tfin_g - t0_g)*3600.0/dt_db_g+1 # based on time of [sec]\n",
    "    nts_db_g=int(nts_db_g)\n",
    "    dbcond={dbfksegID:{'elev':np.zeros(nts_db_g)\n",
    "                      , 'discharge':np.zeros(nts_db_g)} } \n",
    "    \n",
    "    #** add one more fake segment to each varialble **\n",
    "    for x in range(network['maximum_reach_seqorder'],-1,-1):  \n",
    "        for head_segment, reach in ordered_reaches[x]:      \n",
    "            fksegID=fksegID_dsend[head_segment]   \n",
    "            \n",
    "            #ydaq_ev.update({fksegID:{'elev':np.zeros(ntss_ev_g)\n",
    "            #                        ,'q':np.zeros(ntss_ev_g)\n",
    "            #                        ,'celerity':np.zeros(ntss_ev_g)\n",
    "            #                        ,'diffusivity':np.zeros(ntss_ev_g)}})\n",
    "\n",
    "            qlatral.update({fksegID:{'qlat':np.zeros(nts_ql_g)}})\n",
    "\n",
    "   \n",
    "    #-----------------------------------------------------------------------------------------\n",
    "    #                                       Step 1\n",
    "\n",
    "    # Set boundary data such as discharge, water depth, lateral flow by either data import or \n",
    "    # artificial equations.       \n",
    "    # ** the unit of qlat that is eventually used in the routing SHOULD BE [m^2/sec].\n",
    "    # ** qlat of fake segment of the last segment of a reach is always ZERO.\n",
    "    #-----------------------------------------------------------------------------------------            \n",
    "    #setbd.set_bdrydata(        \n",
    "    #                connections= connections  \n",
    "    #                , supernetwork_data= supernetwork_data\n",
    "    #                , network= network\n",
    "    #                , ordered_reaches= ordered_reaches\n",
    "    #                , seg_list_all= seg_list_all\n",
    "    #                , ncompall= ncompall \n",
    "    #                , fksegID_dsend= fksegID_dsend\n",
    "    #                , dbfksegID= dbfksegID\n",
    "    #                , nts_ql_g= nts_ql_g\n",
    "    #                , nts_ub_g= nts_ub_g\n",
    "    #                , nts_db_g= nts_db_g                    \n",
    "    #                , bdrydata= \"artf_data1\"\n",
    "    #                , z_all= z_all\n",
    "    #                , ubcond= ubcond\n",
    "    #                , dbcond= dbcond \n",
    "    #                , qlatral= qlatral\n",
    "    #                )  \n",
    "    qlrtv.retrieve_qlatral(        \n",
    "                            connections= connections \n",
    "                            , supernetwork_data= supernetwork_data\n",
    "                            , network= network\n",
    "                            , ordered_reaches= ordered_reaches\n",
    "                            , seg_list_all= seg_list_all\n",
    "                            , ncompall= ncompall\n",
    "                            , fksegID_dsend= fksegID_dsend\n",
    "                            , nts_ql_g= nts_ql_g # ntsi= ntsi                            \n",
    "                            , custom_input_folder = custom_input_folder\n",
    "                            , custom_input_file= custom_input_file\n",
    "                            , custom_input_file_path= custom_input_file_path\n",
    "                            , output_path= output_path\n",
    "                            , qlatral= qlatral\n",
    "                            )    \n",
    "    # ** qlateral flow mapping\n",
    "    df.var.qlat_g=np.zeros((nts_ql_g, mxncomp_g, nrch_g))    \n",
    "    frj= -1\n",
    "    with open(os.path.join(output_path,\"qlat_ar.txt\"),'a') as pyql:\n",
    "        for x in range(network['maximum_reach_seqorder'],-1,-1):   \n",
    "            for head_segment, reach in ordered_reaches[x]:                  \n",
    "                seg_list= seg_list_all[head_segment] \n",
    "                ncomp=ncompall[head_segment]              \n",
    "                frj= frj+1     \n",
    "                for seg in range(0,ncomp):                               \n",
    "                    segID= seg_list[seg]\n",
    "                    for tsi in range (0,nts_ql_g):\n",
    "                        df.var.qlat_g[tsi,seg,frj]= qlatral[segID]['qlat'][tsi] \n",
    "                        t_min= t0_g*60.0 + dt_ql_g/60.0*float(tsi)                        \n",
    "                        pyql.write(\"%s %s %s %s\\n\" %\\\n",
    "                                (frj+1, seg+1, t_min,df.var.qlat_g[tsi,seg,frj]))  \n",
    "                                # <- +1 is added to frj for accommodating Fortran indexing.\n",
    "                \n",
    "    setbd.set_bdrydata_usgs1(        \n",
    "                    connections= connections  \n",
    "                    , supernetwork_data= supernetwork_data\n",
    "                    , network= network\n",
    "                    , ordered_reaches= ordered_reaches\n",
    "                    , seg_list_all= seg_list_all\n",
    "                    , ncompall= ncompall \n",
    "                    , fksegID_dsend= fksegID_dsend\n",
    "                    , dbfksegID= dbfksegID\n",
    "                    , dbseg_usgsID= dbseg_usgsID\n",
    "                    , nts_ql_g= nts_ql_g\n",
    "                    , nts_ub_g= nts_ub_g\n",
    "                    , nts_db_g= nts_db_g\n",
    "                    , ubcond= ubcond\n",
    "                    , dbcond= dbcond  \n",
    "                    , usgssDT= usgssDT\n",
    "                    , usgseDT= usgseDT\n",
    "                    , qlatral= qlatral\n",
    "                    , output_path= output_path\n",
    "                    )       \n",
    "                                \n",
    "    # ** upstream boundary mapping\n",
    "    df.var.ubcd_g=np.zeros((nts_ub_g, nrch_g))  \n",
    "    nl_ubcd_ar_g=0\n",
    "    frj=-1\n",
    "    with open(os.path.join(output_path,\"ubcd_ar.txt\"),'a') as ub:\n",
    "        for x in range(network['maximum_reach_seqorder'],-1,-1):       \n",
    "            for head_segment, reach in ordered_reaches[x]:                  \n",
    "                frj=frj+1\n",
    "                # ** headwater reach or upstream reaches\n",
    "                if reach['upstream_reaches']=={0}:\n",
    "                    nrow=0\n",
    "                    for tsi in range(0,nts_ub_g):\n",
    "                        df.var.ubcd_g[tsi, frj]= ubcond[head_segment]['discharge'][tsi]\n",
    "                        nl_ubcd_ar_g= nl_ubcd_ar_g+1\n",
    "                        # write to file\n",
    "                        t_min= t0_g*60.0 + dt_ub_g/60.0*float(tsi)                        \n",
    "                        nrow=nrow+1 #<- for fortran row index\n",
    "                        ub.write(\"%s %s %s %s\\n\" %\\\n",
    "                            (frj+1, nrow, t_min, df.var.ubcd_g[tsi, frj]))  \n",
    "    print(f\"nl_ubcd_ar_g:{nl_ubcd_ar_g}\")                    \n",
    "    \n",
    "    \n",
    "    # ** downstream boundary mapping\n",
    "    df.var.dbcd_g=np.zeros(nts_db_g)\n",
    "    with open(os.path.join(output_path,\"dbcd_ar.txt\"),'a') as db:\n",
    "        for tsi in range(0,nts_db_g):\n",
    "            df.var.dbcd_g[tsi]= dbcond[dbfksegID]['elev'][tsi]\n",
    "            # write to file\n",
    "            t_min= t0_g*60.0 + dt_db_g/60.0*float(tsi)                        \n",
    "            db.write(\"%s %s\\n\" %\\\n",
    "                    (t_min, df.var.dbcd_g[tsi]))  \n",
    "    \n",
    "    #import pdb; pdb.set_trace()\n",
    "    #------------------------------------------------------------------------------------\n",
    "    #                                   Step 2\n",
    "\n",
    "    #    Intial condition of q (p.130,rm4)\n",
    "    #------------------------------------------------------------------------------------\n",
    "#    dfrt.icond_q(          \n",
    "#                connections= connections\n",
    "#                , supernetwork_data= supernetwork_data\n",
    "#                , network= network\n",
    "#                , ordered_reaches= ordered_reaches\n",
    "#                , fksegID_dsend= fksegID_dsend\n",
    "#                , seg_list_all= seg_list_all\n",
    "#                , ncompall= ncompall\n",
    "#                , z_all= z_all\n",
    "#                , ubcond=ubcond \n",
    "#                , ydaq= ydaq\n",
    "#                , qlatral= qlatral\n",
    "#                ) \n",
    "#    df.var.q_g=np.zeros((ntss, mxncomp_g, nrch_g)) \n",
    "#    frj= -1\n",
    "#    for x in range(network['maximum_reach_seqorder'],-1,-1):   \n",
    "#        for head_segment, reach in ordered_reaches[x]:                  \n",
    "#            seg_list= seg_list_all[head_segment] \n",
    "#            ncomp=ncompall[head_segment]              \n",
    "#            frj= frj+1     \n",
    "#            for seg in range(0,ncomp):                               \n",
    "#                segID= seg_list[seg]            \n",
    "#                df.var.q_g[0,seg,frj]= ydaq[segID]['q'][0] \n",
    " \n",
    "    #import pdb; pdb.set_trace()\n",
    "    #-------------------------------------------------------------------------------------\n",
    "    #                                     Step 3\n",
    "\n",
    "    #       Run diffusive routing based on Crank-Nicholson & Hermite Interpolation\n",
    "    #         \n",
    "    #-------------------------------------------------------------------------------------    \n",
    "    \n",
    "    # ** set variables in Fortran var module\n",
    "    df.var.nel_g=nel_g   \n",
    "    #df.var.nxsecpt_g = nxsecpt_g  <- used only locally\n",
    "    \n",
    "    df.var.dtini_g =dtini_g \n",
    "    df.var.t0_g = t0_g        # simulation starting time in hr\n",
    "    df.var.tfin_g = tfin_g    # simulation ending time in hr\n",
    "    df.var.cfl_g = cfl_g      # max. allowable Courant number \n",
    "    #df.var.saveinterval_g = saveinterval_g  # output publish time interval in sec  # <- not used\n",
    "    df.var.saveinterval_ev_g = saveinterval_ev_g # <- not used\n",
    "    #xcs_g, ycs_g <- used only locally \n",
    "    #xsec_attr_seg_g(:,:) <- used only locally \n",
    "    \n",
    "    #df.var.xsec_attr_rch_g=np.zeros((mxncomp_g, nrch_g, nel_g, 4)) <- this table set to not be used.\n",
    "    #df.var.xsec_attr_rch_g= xsec_attr_rch_g\n",
    "    \n",
    "    #timesDepth_g <- used only locally\n",
    "    tzeq_flag_g=1\n",
    "    df.var.tzeq_flag_g= tzeq_flag_g\n",
    "    #z_g, bo_g, traps_g, tw_g, twcc_g, So_g, mann_g, manncc_g  <- used only locally \n",
    "    \n",
    "    dx_mn_g=min([connections[segID]['data'][supernetwork_data['length_col']] for segID in connections]) \n",
    "    df.var.dx_mn_g=dx_mn_g         # minimum length value among all segments of a network\n",
    "    \n",
    "    theta_g=1.0\n",
    "    df.var.theta_g= theta_g\n",
    "    \n",
    "    y_opt_g = 1  # 1 for normal depth(kinematic); 2 for dept of diffusive wave.\n",
    "    df.var.y_opt_g= y_opt_g  \n",
    "    \n",
    "    df.var.mxncomp_g= mxncomp_g\n",
    "    df.var.nrch_g= nrch_g\n",
    "    \n",
    "    df.var.nts_ql_g= nts_ql_g   \n",
    "    df.var.nts_ub_g= nts_ub_g\n",
    "    df.var.nts_db_g= nts_db_g \n",
    "    df.var.ntss_ev_g= ntss_ev_g\n",
    "    \n",
    "    df.var.nhincr_m_g= nhincr_m_g\n",
    "    df.var.nhincr_f_g= nhincr_f_g       \n",
    "    \n",
    "    df.var.dt_ql_g= dt_ql_g\n",
    "    df.var.dt_ub_g= dt_ub_g\n",
    "    df.var.dt_db_g= dt_db_g \n",
    "    \n",
    "    df.var.ufhlt_m_g= np.zeros((mxncomp_g,nrch_g,nhincr_m_g))    \n",
    "    df.var.ufqlt_m_g= np.zeros((mxncomp_g,nrch_g,nhincr_m_g))                        \n",
    "    df.var.ufhlt_f_g= np.zeros((mxncomp_g,nrch_g,nhincr_f_g))  \n",
    "    df.var.ufqlt_f_g= np.zeros((mxncomp_g,nrch_g,nhincr_f_g))\n",
    "    \n",
    "    df.var.ufhlt_m_g= ufhlt_m_g \n",
    "    df.var.ufqlt_m_g= ufqlt_m_g\n",
    "    df.var.ufhlt_f_g= ufhlt_f_g\n",
    "    df.var.ufqlt_f_g= ufqlt_f_g\n",
    "\n",
    "    df.var.frnw_g= np.zeros((nrch_g,8), dtype=int)\n",
    "    df.var.frnw_g= frnw_g    \n",
    "\n",
    "    df.var.z_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    df.var.bo_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    df.var.traps_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    df.var.tw_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    df.var.twcc_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    df.var.mann_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    df.var.manncc_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    df.var.so_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    df.var.dx_ar_g= np.zeros((mxncomp_g, nrch_g))\n",
    "    \n",
    "    df.var.z_ar_g= z_ar_g\n",
    "    df.var.bo_ar_g= bo_ar_g\n",
    "    df.var.traps_ar_g= traps_ar_g\n",
    "    df.var.tw_ar_g= tw_ar_g\n",
    "    df.var.twcc_ar_g= twcc_ar_g\n",
    "    df.var.mann_ar_g= mann_ar_g\n",
    "    df.var.manncc_ar_g= manncc_ar_g\n",
    "    df.var.so_ar_g= so_ar_g   \n",
    "    df.var.dx_ar_g= dx_ar_g\n",
    "    \n",
    "    df.var.q_ev_g= np.zeros((ntss_ev_g, mxncomp_g, nrch_g))\n",
    "    df.var.elv_ev_g= np.zeros((ntss_ev_g, mxncomp_g, nrch_g))   \n",
    "\n",
    "    #celty_g, diffty_g, q_g, elv_g, qpx_g<- used only locally \n",
    "    #cel_av_g<- used only locally \n",
    "    #qlatj_g<- used only locally \n",
    "    #eei_g, ffi_g, exi_g, fxi_g<- used only locally \n",
    "    #df.var.diagflg_time=0.5 #* for only print [min]. Temporary\n",
    "    \n",
    "    df.var.so_llm_g= 0.0005  # minimally allowed channel bottom slope\n",
    "\n",
    "    \n",
    "    df.mdiff.routenw()   \n",
    "\n",
    "\n",
    "\n",
    " # Run this function\n",
    "setting_hydTables_Fr_PyMap()  \n",
    "set_run_diff_routing()\n",
    "\n",
    "print('ordered reach computation complete')\n",
    "print(\"... in %s seconds.\" % (time.time() - start_time))\n",
    "\n",
    "#print(f\"main_frnw_g:{frnw_g}\")\n",
    "#print(f\"main_pynw:{pynw}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
